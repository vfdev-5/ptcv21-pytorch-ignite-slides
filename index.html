<!doctype html><html lang=en><head><meta charset=utf-8><title>PTCV21: PyTorch-Ignite</title><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black-translucent"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><link rel=stylesheet href=/ptcv21-pytorch-ignite-slides/reveal-js/css/reset.css><link rel=stylesheet href=/ptcv21-pytorch-ignite-slides/reveal-js/css/reveal.css><link rel=stylesheet href=/ptcv21-pytorch-ignite-slides/_.min.8ec951ac6da9e7125707e4be140a4254d72ce59870ff05ab63614755f666d83d.css id=theme><link rel=stylesheet href=/ptcv21-pytorch-ignite-slides/highlight-js/monokai-sublime.min.css></head><body><style>#logo{position:absolute;top:1%;left:1%;width:15%}</style><img id=logo src=https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logomark.svg alt><div class=reveal><div class=slides><section><h1 id=pytorch-ignite>PyTorch-Ignite</h1><blockquote><p>Training and evaluating neural networks
flexibly and transparently</p></blockquote><p>Victor, Sylvain & Taras</p><div class=container><div class=columns><div class=column><div class=level-left><a class=level-item href=https://www.github.com/pytorch/ignite><span class=icon><i class="fab fa-github"></i></span></a><a class=level-item href=https://www.twitter.com/pytorch_ignite><span class=icon><i class="fab fa-twitter"></i></span></a><a class=level-item href=https://www.facebook.com/PyTorch-Ignite-Community-105837321694508><span class=icon><i class="fab fa-facebook"></i></span></a><a class=level-item href=https://dev.to/pytorch-ignite><span class=icon><i class="fab fa-dev"></i></span></a><a class=level-item href=https://discord.gg/djZtm3EmKj><span class=icon><i class="fab fa-discord"></i></span></a></div></div></div></div><p><div style=color:transparent>.</div><div style=color:transparent>.</div></p><p>Slides: <a href=https://vfdev-5.github.io/ptcv21-pytorch-ignite-slides>https://vfdev-5.github.io/ptcv21-pytorch-ignite-slides</a></p></section><section><h1 id=content>Content</h1><ul><li>PyTorch-Ignite: what and why?</li><li>Quick-start example</li><li>Convert PyTorch to Ignite</li><li>About the project</li></ul></section><section><section data-shortcode-section><h1 id=pytorch-ignite-what-and-why->PyTorch-Ignite: what and why? ü§î</h1><blockquote><p>High-level <strong>library</strong> to help with training and evaluating neural networks in PyTorch flexibly and transparently.</p></blockquote><ul><li><a href=https://github.com/pytorch/ignite>https://github.com/pytorch/ignite</a></li></ul><table style=font-size:20px><tr><td><pre><code class=language-python>
def train_step(engine, batch):
  #  ... any training logic ...
  return batch_loss

trainer = Engine(train_step)

# Compose your pipeline ...

trainer.run(train_loader, max_epochs=100)


</code></pre></td><td><pre><code class=language-python>
metrics = {
  &quot;precision&quot;: Precision(),
  &quot;recall&quot;: Recall()
}

evaluator = create_supervised_evaluator(
  model,
  metrics=metrics
)


</code></pre></td><td><pre><code class=language-python>@trainer.on(Events.EPOCH_COMPLETED)
def run_evaluation():
  evaluator.run(test_loader)

handler = ModelCheckpoint(
  '/tmp/models', 'checkpoint'
)
trainer.add_event_handler(
  Events.EPOCH_COMPLETED,
  handler,
  {'model': model}
)
</code></pre></td></tr></table></section><section><h1 id=what-makes-pytorch-ignite-unique->What makes PyTorch-Ignite unique ?</h1><ul><li>Composable and interoperable components</li><li>Simple and understandable code</li><li>Open-source community involvement</li></ul></section><section><h1 id=key-concepts-in-a-nutshell>Key concepts in a nutshell</h1><h4 id=pytorch-ignite-is-about>PyTorch-Ignite is about:</h4><ol><li>Engine and Event System</li><li>Out-of-the-box metrics to easily evaluate models</li><li>Built-in handlers to compose training pipeline</li><li>Distributed Training support</li></ol></section><section><h2 id=engine-and-event-system>Engine and Event System</h2><table style=font-size:20px><tr><td><div style=color:transparent>.</div><ul><li><p><strong>Engine</strong></p><ul><li>Loops on user data</li><li>Applies an arbitrary user function on batches</li></ul></li><li><p><strong>Event system</strong></p><ul><li>Customizable event collections</li><li>Triggers handlers attached to events</li></ul></li></ul></td><td>In its simpliest form:<pre><code class=language-python data-line-numbers=2,5,7|1,3,6,8,10,11>fire_event(Events.STARTED)
while epoch &lt; max_epochs:
    fire_event(Events.EPOCH_STARTED)

    for batch in data:
        fire_event(Events.ITERATION_STARTED)
        output = train_step(batch)
        fire_event(Events.ITERATION_COMPLETED)

    fire_event(Events.EPOCH_COMPLETED)
fire_event(Events.COMPLETED)
</code></pre></td></tr></table></section><section><h3 id=simplified-training-and-validation-loop>Simplified training and validation loop</h3><div style=font-size:20px><p>No more coding <code>for/while</code> loops on epochs and iterations. Users instantiate engines and run them.</p><pre><code class=language-python>from ignite.engine import Engine, Events, create_supervised_evaluator
from ignite.metrics import Accuracy


# Setup training engine:
def train_step(engine, batch):
    # Users can do whatever they need on a single iteration
    # Eg. forward/backward pass for any number of models, optimizers, etc.
    # ...

trainer = Engine(train_step)

# Setup single model evaluation engine
evaluator = create_supervised_evaluator(model, metrics={&quot;accuracy&quot;: Accuracy()})

def validation():
    state = evaluator.run(validation_data_loader)
    # print computed metrics
    print(trainer.state.epoch, state.metrics)

# Run model's validation at the end of each epoch
trainer.add_event_handler(Events.EPOCH_COMPLETED, validation)

# Start the training
trainer.run(training_data_loader, max_epochs=100)
</code></pre></div></section><section><h3 id=power-of-events--handlers->Power of Events & Handlers üöÄ</h3><h4 id=1-execute-any-number-of-functions-whenever-you-wish>1. Execute any number of functions whenever you wish</h4><div style=font-size:20px><p>Handlers can be any function: e.g. lambda, simple function, class method, etc.</p><pre><code class=language-python>trainer.add_event_handler(Events.STARTED, lambda _: print(&quot;Start training&quot;))

# attach handler with args, kwargs
mydata = [1, 2, 3, 4]
logger = ...

def on_training_ended(data):
    print(f&quot;Training is ended. mydata={data}&quot;)
    # User can use variables from another scope
    logger.info(&quot;Training is ended&quot;)


trainer.add_event_handler(Events.COMPLETED, on_training_ended, mydata)
# call any number of functions on a single event
trainer.add_event_handler(Events.COMPLETED, lambda engine: print(engine.state.times))

@trainer.on(Events.ITERATION_COMPLETED)
def log_something(engine):
    print(engine.state.output)
</code></pre></div></section><section><h3 id=power-of-events--handlers>Power of Events & Handlers</h3><h4 id=2-built-in-events-filtering-and-stacking>2. Built-in events filtering and stacking</h4><div style=font-size:20px><pre><code class=language-python># run the validation every 5 epochs
@trainer.on(Events.EPOCH_COMPLETED(every=5))
def run_validation():
    # run validation

@trainer.on(Events.COMPLETED | Events.EPOCH_COMPLETED(every=10))
def run_another_validation():
    # ...

# change some training variable once on 20th epoch
@trainer.on(Events.EPOCH_STARTED(once=20))
def change_training_variable():
    # ...

# Trigger handler with customly defined frequency
@trainer.on(Events.ITERATION_COMPLETED(event_filter=first_x_iters))
def log_gradients():
    # ...

</code></pre></div></section><section><h3 id=power-of-events--handlers-1>Power of Events & Handlers</h3><h4 id=3-custom-events-to-go-beyond-standard-events>3. Custom events to go beyond standard events</h4><div style=font-size:20px><pre><code class=language-python>from ignite.engine import EventEnum

# Define custom events
class BackpropEvents(EventEnum):
    BACKWARD_STARTED = 'backward_started'
    BACKWARD_COMPLETED = 'backward_completed'
    OPTIM_STEP_COMPLETED = 'optim_step_completed'

def train_step(engine, batch):
    # ...
    loss = criterion(y_pred, y)
    engine.fire_event(BackpropEvents.BACKWARD_STARTED)
    loss.backward()
    engine.fire_event(BackpropEvents.BACKWARD_COMPLETED)
    optimizer.step()
    engine.fire_event(BackpropEvents.OPTIM_STEP_COMPLETED)
    # ...

trainer = Engine(train_step)
trainer.register_events(*BackpropEvents)

@trainer.on(BackpropEvents.BACKWARD_STARTED)
def function_before_backprop(engine):
    # ...
</code></pre></div></section><section><h1 id=out-of-the-box-metrics->Out-of-the-box metrics üìà</h1><p>50+ distributed ready out-of-the-box metrics to easily evaluate models.</p><ul><li>Dedicated to many Deep Learning tasks</li><li>Easily composable to assemble a custom metric</li><li>Easily extendable to create custom metrics</li></ul><div style=color:transparent>.</div><pre><code class=language-python>precision = Precision(average=False)
recall = Recall(average=False)
F1_per_class = (precision * recall * 2 / (precision + recall))
F1_mean = F1_per_class.mean()  # torch mean method
F1_mean.attach(engine, &quot;F1&quot;)
</code></pre></section><section><h1 id=built-in-handlers>Built-in Handlers</h1><table style=font-size:20px><tr><td><div style=color:transparent>.</div><ul><li>Logging to experiment tracking systems</li><li>Checkpointing,</li><li>Early stopping</li><li>Profiling</li><li>Parameter scheduling</li><li>etc.</li></ul></td><td><pre><code class=language-python># model checkpoint handler
checkpoint = ModelChckpoint('/tmp/ckpts', 'training')
trainer.add_event_handler(Events.EPOCH_COMPLETED(every=2), handler, {'model': model})

# early stopping handler
def score_function(engine):
    val_loss = engine.state.metrics['acc']
    return val_loss
es = EarlyStopping(3, score_function, trainer)
evaluator.add_event_handler(Events.COMPLETED, handler)

# Piecewise linear parameter scheduler
scheduler = PiecewiseLinear(optimizer, 'lr', [(10, 0.5), (20, 0.45), (21, 0.3), (30, 0.1), (40, 0.1)])
trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)

# TensorBoard logger: batch loss, metrics
tb_logger = TensorboardLogger(log_dir=&quot;tb-logger&quot;)
tb_logger.attach_output_handler(
    trainer, event_name=Events.ITERATION_COMPLETED(every=100), tag=&quot;training&quot;,
    output_transform=lambda loss: {&quot;batch_loss&quot;: loss},
)

tb_logger.attach_output_handler(
    evaluator, event_name=Events.EPOCH_COMPLETED,
    tag=&quot;training&quot;, metric_names=&quot;all&quot;,
    global_step_transform=global_step_from_engine(trainer),
)
</code></pre></td></tr></table></section><section><h1 id=distributed-training-support>Distributed Training support</h1><p>Run the same code across all supported backends seamlessly</p><ul><li>Backends from native torch distributed configuration: <code>nccl</code>, <code>gloo</code>, <code>mpi</code></li><li>Horovod framework with <code>gloo</code> or <code>nccl</code> communication backend</li><li>XLA on TPUs via <code>pytorch/xla</code></li></ul><div style=font-size:20px><pre><code class=language-python>import ignite.distributed as idist

def training(local_rank, *args, **kwargs):
    dataloder_train = idist.auto_dataloder(dataset, ...)

    model = ...
    model = idist.auto_model(model)

    optimizer = ...
    optimizer = idist.auto_optimizer(optimizer)

backend = 'nccl'  # or 'gloo', 'horovod', 'xla-tpu' or None
with idist.Parallel(backend) as parallel:
    parallel.run(training)
</code></pre></div></section><section><h1 id=distributed-training-support-1>Distributed Training support</h1><h2 id=distributed-launchers>Distributed launchers</h2><p>Handle distributed launchers with the same code</p><ul><li><code>torch.multiprocessing.spawn</code></li><li><code>torch.distributed.launch</code></li><li><code>horovodrun</code></li><li><code>slurm</code></li></ul></section><section><h1 id=distributed-training-support-2>Distributed Training support</h1><h2 id=unified-distributed-api>Unified Distributed API</h2><ul><li><p>High-level helper methods</p><ul><li><code>idist.auto_model()</code></li><li><code>idist.auto_optim()</code></li><li><code>idist.auto_dataloader()</code></li></ul></li><li><p>Collective operations</p><ul><li><code>all_reduce</code>, <code>all_gather</code>, and more</li></ul></li></ul></section><section><h1 id=the-big-picture>The Big Picture</h1><img height=500 src=images/Ignite_Big_Picture.png></section></section><section><section data-shortcode-section><h1 id=quick-start-example->Quick-start example üë©‚Äçüíªüë®‚Äçüíª</h1><p>Let&rsquo;s train a MNIST classifier with PyTorch-Ignite!</p></section><section><h3 id=installation>Installation</h3><p>With <code>pip</code>:</p><pre><code class=language-bash>$ pip install pytorch-ignite
</code></pre><p>or with <code>conda</code>:</p><pre><code class=language-bash>$ conda install ignite -c pytorch
</code></pre></section><section><h3 id=import-import-import>Import, import, import&mldr;</h3><pre><code class=language-python data-line-numbers=1-6|8-12>import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision.models import resnet18
from torchvision.transforms import Compose, Normalize, ToTensor

from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator
from ignite.metrics import Accuracy, Loss
from ignite.handlers import ModelCheckpoint
from ignite.contrib.handlers import TensorboardLogger
</code></pre></section><section><h3 id=start-with-a-pytorch-code>Start with a PyTorch code</h3><div style=font-size:22px>Set up the dataflow, define a model (adapted ResNet18), a loss and an optimizer.<pre><code class=language-python data-line-numbers=1-7|9-20|22-23>data_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])

train_dataset = MNIST(download=True, root=&quot;.&quot;, transform=data_transform, train=True)
val_dataset = MNIST(download=True, root=&quot;.&quot;, transform=data_transform, train=False)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)

class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.model = resnet18(num_classes=10)
        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)

    def forward(self, x):
        return self.model(x)

device = &quot;cuda&quot;
model = Net().to(device)

optimizer = torch.optim.RMSprop(model.parameters(), lr=0.005)
criterion = nn.CrossEntropyLoss()
</code></pre></div></section><section><h3 id=here-goes-pytorch-ignite>Here goes PyTorch-Ignite!</h3><pre><code class=language-python data-line-numbers=1|3-6|8|9>trainer = create_supervised_trainer(model, optimizer, criterion, device)

val_metrics = {
    &quot;accuracy&quot;: Accuracy(),
    &quot;loss&quot;: Loss(criterion)
}

train_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)
val_evaluator = create_supervised_evaluator(model, metrics=val_metrics, device=device)
</code></pre><ul><li><code>trainer</code> engine to train the model</li><li><code>train_evaluator</code> engine to compute metrics on training set</li><li><code>val_evaluator</code> engine to compute metrics on validation set + save the best models</li></ul></section><section><h4 id=add-handlers-for-logging-the-progress>Add handlers for logging the progress</h4><div style=font-size:26px><pre><code class=language-python data-line-numbers=1-3|5-11|13-19>@trainer.on(Events.ITERATION_COMPLETED(every=100))
def log_training_loss(engine):
    print(f&quot;Epoch[{engine.state.epoch}], Iter[{engine.state.iteration}] Loss: {engine.state.output:.2f}&quot;)

@trainer.on(Events.EPOCH_COMPLETED)
def log_training_results(trainer):
    train_evaluator.run(train_loader)
    metrics = train_evaluator.state.metrics
    print(f&quot;Training Results - Epoch[{trainer.state.epoch}] &quot;
          f&quot;Avg accuracy: {metrics['accuracy']:.2f} &quot;
          f&quot;Avg loss: {metrics['loss']:.2f}&quot;)

@trainer.on(Events.EPOCH_COMPLETED)
def log_validation_results(trainer):
    val_evaluator.run(val_loader)
    metrics = val_evaluator.state.metrics
    print(f&quot;Validation Results - Epoch[{trainer.state.epoch}] &quot;
          f&quot;Avg accuracy: {metrics['accuracy']:.2f} &quot;
          f&quot;Avg loss: {metrics['loss']:.2f}&quot;)
</code></pre></div></section><section><h4 id=add-modelcheckpoint-handler-with-accuracy-as-a-score-function>Add <code>ModelCheckpoint</code> handler with accuracy as a score function</h4><pre><code class=language-python>model_checkpoint = ModelCheckpoint(
    &quot;checkpoint&quot;,
    n_saved=2,
    filename_prefix=&quot;best&quot;,
    score_function=lambda e: e.state.metrics[&quot;accuracy&quot;],
    score_name=&quot;accuracy&quot;,
)

val_evaluator.add_event_handler(Events.COMPLETED, model_checkpoint, {&quot;model&quot;: model})
</code></pre></section><section><h4 id=add-tensorboard-logger>Add Tensorboard Logger</h4><div style=font-size:22px><pre><code class=language-python>tb_logger = TensorboardLogger(log_dir=&quot;tb-logger&quot;)

tb_logger.attach_output_handler(
    trainer,
    event_name=Events.ITERATION_COMPLETED(every=100),
    tag=&quot;training&quot;,
    output_transform=lambda loss: {&quot;batch_loss&quot;: loss},
)

for tag, evaluator in [(&quot;training&quot;, train_evaluator), (&quot;validation&quot;, val_evaluator)]:
    tb_logger.attach_output_handler(
        evaluator,
        event_name=Events.EPOCH_COMPLETED,
        tag=tag,
        metric_names=&quot;all&quot;,
        global_step_transform=global_step_from_engine(trainer)
    )
</code></pre></div></section><section><h4 id=liftoff>üöÄLiftoff!üöÄ</h4><pre><code class=language-python>trainer.run(train_loader, max_epochs=5)
</code></pre><pre><code class=language-python>Epoch[1], Iter[100] Loss: 0.19
Epoch[1], Iter[200] Loss: 0.13
Epoch[1], Iter[300] Loss: 0.08
Epoch[1], Iter[400] Loss: 0.11
Training Results - Epoch[1] Avg accuracy: 0.97 Avg loss: 0.09
Validation Results - Epoch[1] Avg accuracy: 0.97 Avg loss: 0.08
...
Epoch[5], Iter[1900] Loss: 0.02
Epoch[5], Iter[2000] Loss: 0.11
Epoch[5], Iter[2100] Loss: 0.05
Epoch[5], Iter[2200] Loss: 0.02
Epoch[5], Iter[2300] Loss: 0.01
Training Results - Epoch[5] Avg accuracy: 0.99 Avg loss: 0.02
Validation Results - Epoch[5] Avg accuracy: 0.99 Avg loss: 0.03
</code></pre></section><section><h3 id=complete-code>Complete code</h3><p><a href=https://pytorch-ignite.ai/tutorials/getting-started>https://pytorch-ignite.ai/tutorials/getting-started</a></p></section><section><h3 id=pytorch-ignite-code-generator>PyTorch-Ignite Code-Generator</h3><img height=300 src=https://raw.githubusercontent.com/pytorch-ignite/code-generator/main/src/assets/code-generator-demo-1080p.gif><div style=font-size:20px><p><a href=https://code-generator.pytorch-ignite.ai/>https://code-generator.pytorch-ignite.ai/</a></p><ul><li><p><strong>What is Code-Generator?</strong>: web app to quickly produce quick-start python code for common training tasks in deep learning.</p></li><li><p><strong>Why to use Code-Generator?</strong>: start working on a task without rewriting everything from scratch.</p></li></ul></div></section></section><section><section data-shortcode-section><h1 id=-convert-pytorch-to-ignite->üî• Convert PyTorch to Ignite ‚ù§Ô∏è‚Äçüî•</h1><p>How to translate pure PyTorch code to PyTorch+Ignite</p></section><section><img height=600 src=images/1.png></section><section><img height=600 src=images/2.png></section><section><img height=600 src=images/3.png></section><section><img height=600 src=images/4.png></section><section><img height=600 src=images/5.png></section><section><img height=600 src=images/6.png></section><section><img height=600 src=images/7.png></section><section><img height=600 src=images/8.png></section><section><img height=600 src=images/9.png></section><section><img height=600 src=images/10.png></section></section><section><section data-shortcode-section><h1 id=about-pytorch-ignite-project>About &ldquo;PyTorch-Ignite&rdquo; project</h1><p>Community-driven open source and <em>NumFOCUS Affiliated</em> Project</p><p>maintained by volunteers in the PyTorch community:</p><pre><code>@vfdev-5, @ydcjeff, @KickItLikeShika, @sdesrozis, @alykhantejani, @anmolsjoshi,
@trsvchn, @Moh-Yakoub, ..., @fco-dv, @gucifer, @Priyansi, ...
</code></pre><p><img src=https://a.slack-edge.com/production-standard-emoji-assets/13.0/apple-medium/1f389@2x.png alt=o1>
<img src=https://a.slack-edge.com/production-standard-emoji-assets/13.0/apple-medium/1f44f@2x.png alt=o2>
<img src=https://a.slack-edge.com/production-standard-emoji-assets/13.0/apple-medium/1f64f@2x.png alt=o3></p><p>With the support of:</p><img width=200 src=https://numfocus.org/wp-content/uploads/2017/07/NumFocus_LRG.png>
<img width=200 src=https://raw.githubusercontent.com/Quansight-Labs/quansight-labs-site/master/files/images/QuansightLabs_logo_V2.png>
<img width=200 src=https://pytorch-ignite.ai/images/logos/ifpen.jpg></section><section><h1 id=projects-using-pytorch-ignite>Projects using PyTorch-Ignite</h1><ul><li>Research Papers</li><li>Blog articles, tutorials, books</li><li>Toolkits<ul><li><a href=https://monai.io/>Project MONAI</a>, <a href=https://nussl.github.io/docs/>Nussl</a>, &mldr;</li></ul></li></ul><p>More details here: <a href=https://pytorch-ignite.ai/ecosystem/>https://pytorch-ignite.ai/ecosystem/</a></p></section><section><h1 id=community-engagement>Community Engagement</h1><div style=font-size:24px><ul><li><p>Google Summer of Code 2021</p><ul><li>Mentored two great students (<em>Ahmed</em> and <em>Arpan</em>)</li></ul></li><li><p>Google Season of Docs 2021</p><ul><li>Working with great tech writer (<em>Priyansi</em>)</li></ul></li><li><p>Hacktoberfest 2020 and coming up 2021</p></li><li><p>PyData Global Mentored Sprint 2020</p></li><li><p>Our <a href=https://pytorch-ignite.ai>new website</a> development (thanks to <em>Jeff Yang</em>!)</p></li><li><p><a href=https://code-generator.pytorch-ignite.ai>PyTorch-Ignite Code-Generator project</a></p></li></ul><p><em>Stay tuned for upcoming events &mldr;</em></p><img width=50 src=https://summerofcode.withgoogle.com/static/img/summer-of-code-logo.svg>
<img width=50 src=https://developers.google.com/season-of-docs/images/SeasonofDocs_Icon_Grey_300ppi_trimmed_480.png>
<img width=50 src=https://hacktoberfestswaglist.com/img/Hacktoberfest_20.jpg>
<img width=150 src=https://pydata.org/global2021/wp-content/uploads/2021/06/logo.png></div></section><section><h1 id=join-the-pytorch-ignite-community>Join the PyTorch-Ignite Community</h1><p>We are looking for motivated contributors to help out with the project.</p><p><img src=https://a.slack-edge.com/production-standard-emoji-assets/13.0/apple-small/1f3c5@2x.png alt=o1>
Everyone is welcome to contribute
<img src=https://a.slack-edge.com/production-standard-emoji-assets/13.0/apple-small/1f4af@2x.png alt=o2></p><a class=level-item href=https://www.github.com/pytorch/ignite><span class=icon><i class="fab fa-github"></i></span></a><a class=level-item href=https://discord.gg/djZtm3EmKj><span class=icon><i class="fab fa-discord"></i></span></a><div style=color:transparent>.</div><h4 id=how-to-start>How to start:</h4><ul><li>Read our <a href=https://github.com/pytorch/ignite/blob/master/CONTRIBUTING.md>Contributing guides</a></li><li>Pick <a href="https://github.com/pytorch/ignite/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">List of Help-wanted GH issue</a></li><li>Reach out to us on GH or Discord for more guidance</li></ul></section></section><section><table><tr><td><h2 id=thanks-for-watching>Thanks for watching</h2><h2 id=and-listening->and listening !</h2><p><em>Questions?</em></p><p>üôãüë©‚Äçüíªüôãüë®‚Äçüíªüë©‚Äçüíª</p></td><td style="border:1px solid #000"><p>Follow us on</p><a class=level-item href=https://www.twitter.com/pytorch_ignite><span class=icon><i class="fab fa-twitter"></i></span></a><a class=level-item href=https://www.facebook.com/PyTorch-Ignite-Community-105837321694508><span class=icon><i class="fab fa-facebook"></i></span></a><a class=level-item href=https://dev.to/pytorch-ignite><span class=icon><i class="fab fa-dev"></i></span></a><p>and check out our new website:</p><p><a href=https://pytorch-ignite.ai>https://pytorch-ignite.ai</a></p></td></tr></table></section></div><style type=text/css>#footer-left{position:absolute;bottom:0%;left:50%;margin-right:-50%;transform:translate(-50%,-50%)}</style><footer class=footer>PyTorch Community Voices - 2021/09/15 ‚óã pytorch-ignite.ai</footer></div><script type=text/javascript src=/ptcv21-pytorch-ignite-slides/reveal-hugo/object-assign.js></script><a href=/ptcv21-pytorch-ignite-slides/reveal-js/css/print/ id=print-location style=display:none></a><script type=text/javascript>var printLocationElement=document.getElementById('print-location');var link=document.createElement('link');link.rel='stylesheet';link.type='text/css';link.href=printLocationElement.href+(window.location.search.match(/print-pdf/gi)?'pdf.css':'paper.css');document.getElementsByTagName('head')[0].appendChild(link);</script><script type=application/json id=reveal-hugo-site-params>{"custom_theme":"pytorch-ignite-theme.scss","custom_theme_compile":true,"height":720,"highlight_theme":"monokai-sublime","slide_number":true,"transition_speed":"fast","width":1280}</script><script type=application/json id=reveal-hugo-page-params>null</script><script src=/ptcv21-pytorch-ignite-slides/reveal-js/js/reveal.js></script><script type=text/javascript>function camelize(map){if(map){Object.keys(map).forEach(function(k){newK=k.replace(/(\_\w)/g,function(m){return m[1].toUpperCase()});if(newK!=k){map[newK]=map[k];delete map[k];}});}
return map;}
var revealHugoDefaults={center:true,controls:true,history:true,progress:true,transition:"slide"};var revealHugoSiteParams=JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);var revealHugoPageParams=JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);var options=Object.assign({},camelize(revealHugoDefaults),camelize(revealHugoSiteParams),camelize(revealHugoPageParams));Reveal.initialize(options);</script><script type=text/javascript src=/ptcv21-pytorch-ignite-slides/reveal-js/plugin/markdown/marked.js></script><script type=text/javascript src=/ptcv21-pytorch-ignite-slides/reveal-js/plugin/markdown/markdown.js></script><script type=text/javascript src=/ptcv21-pytorch-ignite-slides/reveal-js/plugin/highlight/highlight.js></script><script type=text/javascript src=/ptcv21-pytorch-ignite-slides/reveal-js/plugin/zoom-js/zoom.js></script><script type=text/javascript src=/ptcv21-pytorch-ignite-slides/reveal-js/plugin/notes/notes.js></script><style>#logo{position:absolute;top:20px;left:20px;width:150px}</style><img id=logo src=https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logomark.svg alt>
<link rel="shortcut icon" href=https://raw.githubusercontent.com/pytorch/ignite/master/assets/logo/ignite_logomark.svg><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css><link rel=stylesheet href=https://rsms.me/inter/inter.css></body></html>